# Alignment
- [2025/03]**[Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards
](https://arxiv.org/abs/2503.11240)** ![arXiv](https://img.shields.io/badge/arXiv-blue) ![Code](https://img.shields.io/badge/Code-violet)
- [2025/03]**[Aligning Text-to-Image Diffusion Models without Human Feedback
](https://ieeexplore.ieee.org/abstract/document/10888279)** ![ICASSP](https://img.shields.io/badge/ICASSP-blue) 
- [2025/03]**[Detect-and-Guide: Self-regulation of Diffusion Models for Safe Text-to-Image Generation via Guideline Token Optimization
](https://arxiv.org/abs/2503.15197)** ![arXiv](https://img.shields.io/badge/arXiv-blue)
- [2025/03]**[CAS: A PROBABILITY-BASED APPROACH FOR UNIVERSAL CONDITION ALIGNMENT SCORE](https://openreview.net/forum?id=E78OaH2s3f)** ![ICLR 24](https://img.shields.io/badge/ICLR%2024-blue) ![Code](https://img.shields.io/badge/Code-violet)
- 
