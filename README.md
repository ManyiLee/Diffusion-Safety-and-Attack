# Diffusion-Safety-and-Attack 

![Repository](https://img.shields.io/badge/Advancement-DSA-red)
![Stars](https://img.shields.io/github/stars/ManyiLee/Diffusion-Safety-and-Attack)

## Introduction 
Thanks to [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP), this repository is built based on part of their collections and progressively include our recent collected resources. In this repositorty, we only focus on resources related to the trustworthiness of image generation model, specifically diffusion models (DMs) across multiple dimensions (e.g., safety, security, and privacy).

- This repo is in progress :point_right: (manually collected).
- Badges: 
    - Paper ![arXiv](https://img.shields.io/badge/arXiv-blue): Blue badges represente where publish this paper.
    - Code ![Code](https://img.shields.io/badge/Code-violet): Violet badge representes if papers have released their code. We recomend you use [Paper with Code](https://paperswithcode.com/) to search open-source repository of papers.
    - Pretrained weight ![Pretrain weight](https://img.shields.io/badge/Pretrain%20weight-important): Important bandage representes if papers have released pretrained weight used in thier experiments

Inclusion :email:: Welcome to recommend resources to us via sending email(string1313@qq.com) or opening issues with the following format: 

| Paper Title | Publish | Paper Link  | Code link | Pretrained weight link |Classification | Further Comments | 
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| ABCDE | CVPR 25 | https:xxx | https:xxx  |  https:xxx | Safety/Jailbreak | Benchmark| 

## News
- [2025.03.09] :boom: DSA is released!

## Collections
- Paper (1463)
    - A. Safety (805)
        - [A0. General](Safety/General.md) (22)
        - [A1. Jailbreak](Safety/JailBreak.md) (340)
        - [A2. Alignment](Safety/Alignment.md) (88)
        - [A3. Deepfake](Safety/Deepfake.md) (64)
        - [A4. Ethics](Safety/Ethics.md) (5)
        - [A5. Fairness](Safety/Fairness.md) (54)
        - [A6. Hallucination](Safety/Hallucination.md) (109)
        - [A7. Prompt Injection](Safety/PromptJnjection.md) (49)
        - [A8. Toxicity](Safety/Toxicity.md) (74)
    - B. Security (220)
        - [B0. General](Security/General.md) (13)
        - [B1. Adversarial Examples](Security/Adversarial_examples.md) (89)
        - [B2. Poison & Backdoor](Security/Poison_&_Backdoor.md) (105)
        - [B3. System](Security/System.md) (13)
    - C. Privacy (438)
        - [C0. General](Privacy/General.md) (31)
        - [C1. Contamination](Privacy/Contamination.md) (13)
        - [C2. Copyright](Privacy/Copyright.md) (153)
        - [C3. Data Reconstruction](Privacy/Data_Reconstruction.md) (48)
        - [C4. Membership Inference Attacks](Privacy/Membership_inference_attacks.md) (39)
        - [C5. Model Extraction](Privacy/Model_extraction.md) (12)
        - [C6. Privacy-Preserving Computation](Privacy/Privacy-preserving_computation.md) (88)
        - [C7. Property Inference Attacks](Privacy/Property_inference_attacks.md) (3)
        - [C8. Side-Channel](Privacy/side-channel.md) (5)
        - [C9. Unlearning](Privacy/unlearning.md) (46)

## Acknowledgement

- Organizers: Manyi Li (李满毅)

- This project is inspired by [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP).
