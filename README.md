# Diffusion-Safety-and-Attack 

![Repository](https://img.shields.io/badge/Advancement-DSA-red)
![Stars](https://img.shields.io/github/stars/ManyiLee/Diffusion-Safety-and-Attack)

## Introduction 
Thanks to [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP), this repository has been constructed utilizing a portion of their collections and progressively incorporates our recently gathered resources. In this repository, we solely focus on resources pertaining to the trustworthiness of image generation models, particularly diffusion models (DMs), across multiple dimensions such as safety, security, and privacy.

- This repo is in progress :point_right: (manually collected).
- Badges: 
    - Paper ![arXiv](https://img.shields.io/badge/arXiv-blue): Blue badges represente where publish this paper.
    - Code ![Code](https://img.shields.io/badge/Code-violet): Violet badge representes if papers have released their code. We recomend you use [Paper with Code](https://paperswithcode.com/) to search open-source repository of papers.
    - Pretrained weight ![Pretrain weight](https://img.shields.io/badge/Pretrain%20weight-important): Important bandage representes if papers have released pretrained weight used in thier experiments

Inclusion :email:: Welcome to recommend resources to us via sending email(string1313@qq.com) or opening issues with the following format: 

| Paper Title | Publish | Paper Link  | Code link | Pretrained weight link |Classification | Further Comments | 
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| ABCDE | CVPR 25 | https:xxx | https:xxx  |  https:xxx | Safety/Jailbreak | Benchmark| 

## News
- [2025.03.09] :boom: DSA is released!

## Collections
- Paper (417)
    - A. Safety (135)
        - [A0. General](Safety/General.md) (12)
        - [A1. Jailbreak](Safety/JailBreak.md) (31)
        - [A2. Alignment](Safety/Alignment.md) (3)
        - [A3. Deepfake](Safety/Deepfake.md) (53)
        - [A4. Ethics](Safety/Ethics.md) (3)
        - [A5. Fairness](Safety/Fairness.md) (9)
        - [A6. Hallucination](Safety/Hallucination.md) (9)
        - [A7. Prompt Injection](Safety/Prompt_injection.md) (2)
        - [A8. Toxicity](Safety/Toxicity.md) (13)
    - B. Security (83)
        - [B0. General](Security/General.md) (2)
        - [B1. Adversarial Examples](Security/Adversarial_examples.md) (47)
        - [B2. Poison & Backdoor](Security/Poison_&_backdoor.md) (34)
        - [B3. System](Security/System.md) (0)
    - C. Privacy (199)
        - [C0. General](Privacy/General.md) (6)
        - [C1. Contamination](Privacy/Contamination.md) (1)
        - [C2. Copyright](Privacy/Copyright.md) (85)
        - [C3. Data Reconstruction](Privacy/Data_reconstruction.md) (9)
        - [C4. Membership Inference Attacks](Privacy/Membership_inference_attacks.md) (16)
        - [C5. Model Extraction](Privacy/Model_extraction.md) (1)
        - [C6. Privacy Preserving Computation](Privacy/Privacy_preserving_computation.md) (14)
        - [C7. Property Inference Attacks](Privacy/Property_inference_attacks.md) (2)
        - [C9. Unlearning](Privacy/Unlearning.md) (65)

## Acknowledgement

- Organizers: Manyi Li (李满毅)

- This project is inspired by [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP).
