# Diffusion-Safety-and-Attack 

![Repository](https://img.shields.io/badge/Advancement-DSA-red)
![Stars](https://img.shields.io/github/stars/ManyiLee/Diffusion-Safety-and-Attack)

## Introduction 
Thanks to [Awesome-LM-SSP](https://github.com/ThuCCSLab/Awesome-LM-SSP), this repository is built based on part of their collections and progressively include our recent collected resources. In this repositorty, we only focus on resources related to the trustworthiness of image generation model, specifically diffusion models (DMs) across multiple dimensions (e.g., safety, security, and privacy).

- This repo is in progress :point_right: (manually collected).
- Badges: 
    - Paper ![arXiv](https://img.shields.io/badge/arXiv-blue): Blue badges represente where publish this paper.
    - Code ![Code](https://img.shields.io/badge/Code-violet): Violet badge representes if papers have released their code. We recomend you use [Paper with Code](https://paperswithcode.com/) to search open-source repository of papers.
    - Pretrained weight ![Pretrain weight](https://img.shields.io/badge/Pretrain%20weight-important): Important bandage representes if papers have released pretrained weight used in thier experiments

Inclusion :email:: Welcome to recommend resources to us via sending email(string1313@qq.com) or opening issues with the following format: 

| Paper Title | Publish | Paper Link  | Code link | Pretrained weight link |Classification | Further Comments | 
| :----: | :----: | :----: | :----: | :----: | :----: | :----: |
| ABCDE | CVPR 25 | https:xxx | https:xxx  |  https:xxx | Safety/Jailbreak | Benchmark| 

## News
- [2025.03.09] :boom: DSA is released!

## Collections
- Paper (1463)
    - A. Safety (805)
        - [A0. General](Safety/General.md) (22)
        - [A1. Jailbreak](safety/jailbreak.md) (340)
        - [A2. Alignment](collection/paper/safety/alignment.md) (88)
        - [A3. Deepfake](collection/paper/safety/deepfake.md) (64)
        - [A4. Ethics](collection/paper/safety/ethics.md) (5)
        - [A5. Fairness](collection/paper/safety/fairness.md) (54)
        - [A6. Hallucination](collection/paper/safety/hallucination.md) (109)
        - [A7. Prompt Injection](collection/paper/safety/prompt_injection.md) (49)
        - [A8. Toxicity](collection/paper/safety/toxicity.md) (74)
    - B. Security (220)
        - [B0. General](collection/paper/security/general.md) (13)
        - [B1. Adversarial Examples](collection/paper/security/adversarial_examples.md) (89)
        - [B2. Poison & Backdoor](collection/paper/security/poison_&_backdoor.md) (105)
        - [B3. System](collection/paper/security/system.md) (13)
    - C. Privacy (438)
        - [C0. General](collection/paper/privacy/general.md) (31)
        - [C1. Contamination](collection/paper/privacy/contamination.md) (13)
        - [C2. Copyright](collection/paper/privacy/copyright.md) (153)
        - [C3. Data Reconstruction](collection/paper/privacy/data_reconstruction.md) (48)
        - [C4. Membership Inference Attacks](collection/paper/privacy/membership_inference_attacks.md) (39)
        - [C5. Model Extraction](collection/paper/privacy/model_extraction.md) (12)
        - [C6. Privacy-Preserving Computation](collection/paper/privacy/privacy-preserving_computation.md) (88)
        - [C7. Property Inference Attacks](collection/paper/privacy/property_inference_attacks.md) (3)
        - [C8. Side-Channel](collection/paper/privacy/side-channel.md) (5)
        - [C9. Unlearning](collection/paper/privacy/unlearning.md) (46)

## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=ThuCCSLab/Awesome-LM-SSP&type=Date)](https://star-history.com/#ThuCCSLab/Awesome-LM-SSP&Date)

## Acknowledgement

- Organizers: [Tianshuo Cong (丛天硕)](https://tianshuocong.github.io/), [Xinlei He (何新磊)](https://xinleihe.github.io/), [Zhengyu Zhao (赵正宇)](https://zhengyuzhao.github.io/), [Yugeng Liu (刘禹更)](https://liu.ai/), [Delong Ran (冉德龙)](https://github.com/eggry)

- This project is inspired by [LLM Security](https://llmsecurity.net/), [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security), [LLM Security & Privacy](https://github.com/chawins/llm-sp),             [UR2-LLMs](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness), [PLMpapers](https://github.com/thunlp/PLMpapers), [EvaluationPapers4ChatGPT](https://github.com/THU-KEG/EvaluationPapers4ChatGPT)

<p align="center"><img src="figure/logo.png" width="900" /></p>
